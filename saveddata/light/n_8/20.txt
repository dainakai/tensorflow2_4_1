test data loss : 0.693148136138916
test data accyracy : 0.5

training history accuracy
0.5104166865348816
0.49916666746139526
0.4858333468437195
0.4958333373069763
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.4950000047683716
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.4945833384990692
0.5020833611488342
0.4962500035762787
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.492083340883255
0.5020833611488342
0.5020833611488342
0.49125000834465027
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.49541667103767395
0.49041667580604553
0.4908333420753479
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.49291667342185974
0.4908333420753479
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.5020833611488342
0.4950000047683716
0.5020833611488342
0.5020833611488342

training history val accuracy
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264
0.49166667461395264

training history loss
0.6969037652015686
0.6932044625282288
0.6932518482208252
0.6932189464569092
0.6931822896003723
0.6931728720664978
0.6931633353233337
0.6931820511817932
0.6932520270347595
0.6931712031364441
0.6931657791137695
0.6931849718093872
0.6932058930397034
0.6931795477867126
0.6931819915771484
0.6932036876678467
0.6931582093238831
0.6931636333465576
0.6932059526443481
0.6931870579719543
0.6931746602058411
0.6932003498077393
0.6931627988815308
0.6932429671287537
0.6931795477867126
0.6931641697883606
0.6931853294372559
0.6931983232498169
0.6932359337806702
0.6931572556495667
0.6931580305099487
0.693173348903656
0.6931791305541992
0.693173348903656
0.6932116746902466
0.6931881904602051
0.6931808590888977
0.6931794881820679
0.6931664943695068
0.6931656002998352
0.693167507648468
0.6931973695755005
0.6931867599487305
0.6931681632995605
0.6931683421134949

training history val loss
0.6931747794151306
0.6932564377784729
0.6933019161224365
0.6932492852210999
0.6932253241539001
0.6932750344276428
0.6932376623153687
0.6932055950164795
0.6932995319366455
0.6932174563407898
0.6931989192962646
0.6932287216186523
0.6932445764541626
0.6932947039604187
0.6931732892990112
0.6932743787765503
0.6932101845741272
0.693221390247345
0.693220317363739
0.6932763457298279
0.6932010054588318
0.6932280659675598
0.6931934356689453
0.6932172179222107
0.6932372450828552
0.6931996941566467
0.693221926689148
0.6932438611984253
0.6932674646377563
0.6932839751243591
0.6932492256164551
0.6932402849197388
0.6932063102722168
0.6932581663131714
0.6932170391082764
0.6932666301727295
0.6932314038276672
0.6932523846626282
0.6932483911514282
0.6932494044303894
0.6932620406150818
0.6931883096694946
0.6931750178337097
0.6932370066642761
0.6931970119476318